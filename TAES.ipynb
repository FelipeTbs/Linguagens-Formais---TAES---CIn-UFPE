{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeTbs/Linguagens-Formais---TAES---CIn-UFPE/blob/main/TAES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importações de dependências"
      ],
      "metadata": {
        "id": "V3ve30S-I6-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULpAkJeeIx5A"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import time\n",
        "from difflib import SequenceMatcher\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!pip install -q -U google-generativeai openai \"rich<14\"\n",
        "!pip install -q anthropic\n",
        "print(\"Bibliotecas instaladas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Artefatos de Engenharia de prompt"
      ],
      "metadata": {
        "id": "JaGJQXLGJFNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "PROMPT_SPECIALIST_SYSTEM = \"\"\"\n",
        "A seguir haverá uma especificação formal escrita em Alloy. Sua tarefa é interpretar esse\n",
        "modelo e explicá-lo de forma totalmente compreensível para um leitor leigo, mantendo\n",
        "precisão conceitual. Produza a saída em três seções: (1) Resumo Executivo, com 2 a 3\n",
        "frases em linguagem simples e sem jargões, explicando o propósito do modelo e o\n",
        "problema do mundo real que ele representa; (2) Explicação Detalhada da Especificação\n",
        "Formal, descrevendo cada parte do modelo em ordem lógica, explicando o significado de\n",
        "assinaturas, relações, predicados, cardinalidades (como lone, one, some, set),\n",
        "restrições e estrutura semântica. Nesta seção, apresente explicações narrativas\n",
        "passo a passo, exemplos de entradas e saídas esperadas, possíveis interpretações,\n",
        "tabelas estruturadas quando útil, e traduções conceituais para linguagem natural;\n",
        "(3) Pontos Críticos, Restrições e Alertas, listando limitações, ambiguidades,\n",
        "pressupostos implícitos e restrições importantes derivadas do modelo.\n",
        "\n",
        "Regras obrigatórias: utilizar linguagem acessível para alguém não técnico\n",
        "(por exemplo, um analista de negócios), evitar jargões e, quando usados,\n",
        "explicar imediatamente; não apenas traduzir o código, mas interpretar seu\n",
        "significado e finalidade; fornecer contexto prático e clareza sem precisar\n",
        "conhecimento prévio em lógica ou formal methods; organizar a saída de forma\n",
        "clara e legível.\n",
        "\n",
        "O formato final da resposta deve seguir exatamente:\n",
        "\t1.\tResumo Executivo\n",
        "\t2.\tExplicação Detalhada da Especificação Formal\n",
        "\t3.\tPontos Críticos, Restrições e Alertas\n",
        "\n",
        "Agora, aqui está o modelo a ser analisado:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZgvaeZ2LJOsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importação de datasets"
      ],
      "metadata": {
        "id": "ttyJ2KoXJPHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CAMINHO_PASTA = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "def carregar_dataset_filtrado(caminho, limite=5, buscar_nomes=None):\n",
        "    \"\"\"\n",
        "    Carrega apenas 'limite' arquivos.\n",
        "    Se 'buscar_nomes' for passado, tenta achar esses arquivos primeiro.\n",
        "    \"\"\"\n",
        "    print(f\"Lendo pasta: {caminho}\")\n",
        "    todos_arquivos = glob.glob(f\"{caminho}/*.als\")\n",
        "\n",
        "    if len(todos_arquivos) == 0:\n",
        "        print(\"Nenhum arquivo encontrado! Verifique o caminho.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    arquivos_selecionados = []\n",
        "\n",
        "    if buscar_nomes:\n",
        "        for nome_buscado in buscar_nomes:\n",
        "            match = next((f for f in todos_arquivos if nome_buscado in os.path.basename(f)), None)\n",
        "            if match:\n",
        "                arquivos_selecionados.append(match)\n",
        "                print(f\"   --> Encontrado prioritário: {os.path.basename(match)}\")\n",
        "\n",
        "    for arq in todos_arquivos:\n",
        "        if len(arquivos_selecionados) >= limite:\n",
        "            break\n",
        "        if arq not in arquivos_selecionados:\n",
        "            arquivos_selecionados.append(arq)\n",
        "\n",
        "    dados = []\n",
        "    print(f\"\\n Importando {len(arquivos_selecionados)} arquivos selecionados:\")\n",
        "\n",
        "    for arq in arquivos_selecionados:\n",
        "        try:\n",
        "            with open(arq, 'r', encoding='utf-8') as f:\n",
        "                conteudo = f.read()\n",
        "            nome = os.path.basename(arq)\n",
        "            print(f\"   - {nome}\")\n",
        "\n",
        "            dados.append({\n",
        "                \"arquivo\": nome,\n",
        "                \"codigo_alloy\": conteudo,\n",
        "                \"tamanho\": len(conteudo)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao ler {arq}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(dados)\n",
        "\n",
        "\n",
        "PRIORIDADES = ['grandpa2.als', 'genealogy.als', 'addressBook.als', 'filesystem.als', 'ringElection2.als']\n",
        "\n",
        "if os.path.exists('/content/drive'):\n",
        "    df_alloy = carregar_dataset_filtrado(CAMINHO_PASTA, limite=5, buscar_nomes=PRIORIDADES)\n",
        "    display(df_alloy)\n",
        "else:\n",
        "    print(\"Rode a célula para conectar o Drive!\")\n"
      ],
      "metadata": {
        "id": "nfdWZpuwJSrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importação de Modelos"
      ],
      "metadata": {
        "id": "_ICS3X8wJTIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"Chaves recuperadas dos Secrets com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao pegar chaves: {e}\")\n",
        "\n",
        "# Configuração dos modelos\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "client_gpt = OpenAI(api_key=OPENAI_API_KEY)\n",
        "print('Chaves configuradas com sucesso!')\n",
        "\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "\n",
        "client_claude = anthropic.Anthropic(\n",
        "    api_key=ANTHROPIC_API_KEY,\n",
        ")\n",
        "\n",
        "def generalist_model_claude(alloy_code):\n",
        "  try:\n",
        "      final_prompt = f\"{PROMPT_SPECIALIST_SYSTEM}\\n\\n{alloy_code}\"\n",
        "      message = client_claude.messages.create(\n",
        "            model=\"claude-sonnet-4-5-20250929\", # O modelo SOTA atual\n",
        "            max_tokens=2000,\n",
        "            temperature=0.1, # Baixa temperatura para precisão\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": final_prompt}\n",
        "            ]\n",
        "      )\n",
        "      return message.content[0].text\n",
        "\n",
        "  except Exception as e:\n",
        "        return f\"Erro Claude: {e}\"\n",
        "\n",
        "def generalist_model(alloy_code):\n",
        "    \"\"\"GPT-4o (Zero-Shot)\"\"\"\n",
        "    try:\n",
        "        final_prompt = f\"Explique em Português do Brasil o que esse código faz, lembrem que eu não tenho conhecimento em programação e lógica:\\n\\n{alloy_code}\"\n",
        "        response = client_gpt.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": final_prompt}\n",
        "            ],\n",
        "            temperature=0.1 # Baixa temperatura para precisão\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Erro Gemini: {e}\"\n",
        "\n",
        "def specialist_model(alloy_code):\n",
        "    \"\"\"GPT-4o (Few-Shot + System Prompt)\"\"\"\n",
        "    try:\n",
        "        response = client_gpt.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": PROMPT_SPECIALIST_SYSTEM}, #manual de alloy aqui (definir especialidade)\n",
        "                {\"role\": \"user\", \"content\": alloy_code}\n",
        "            ],\n",
        "            temperature=0.1 # Baixa temperatura para precisão\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Erro GPT: {e}\""
      ],
      "metadata": {
        "id": "qBZQNNRuJuFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparação e Resultados"
      ],
      "metadata": {
        "id": "WHlGfA56JqxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lista para guardar os dados\n",
        "resultados_finais = []\n",
        "\n",
        "print(f\"Iniciando processamento de {len(df_alloy)} arquivos...\")\n",
        "\n",
        "for index, row in tqdm(df_alloy.iterrows(), total=df_alloy.shape[0]):\n",
        "    nome_arquivo = row['arquivo']\n",
        "    codigo = row['codigo_alloy']\n",
        "\n",
        "    # --- 1. MODELO GENERALISTA (GPT-4o Zero-Shot) ---\n",
        "    start_gen = time.time()\n",
        "    resp_gen = generalist_model(codigo)\n",
        "    tempo_gen = time.time() - start_gen\n",
        "\n",
        "    # --- 2. MODELO ESPECIALISTA (GPT-4o + System Prompt) ---\n",
        "    start_esp = time.time()\n",
        "    resp_esp = specialist_model(codigo)\n",
        "    tempo_esp = time.time() - start_esp\n",
        "\n",
        "    # (Opcional) --- 3. CLAUDE (Se estiver funcionando) ---\n",
        "    # start_claude = time.time()\n",
        "    # resp_claude = generalist_model_claude(codigo)\n",
        "    # tempo_claude = time.time() - start_claude\n",
        "\n",
        "    # --- SALVAR RESULTADOS ---\n",
        "    resultados_finais.append({\n",
        "        \"Arquivo\": nome_arquivo,\n",
        "        \"Tamanho_Codigo\": len(codigo),\n",
        "\n",
        "        # Dados do Generalista\n",
        "        \"Generalista_Output\": resp_gen,\n",
        "        \"Generalista_Tempo_Sec\": round(tempo_gen, 2),\n",
        "        \"Generalista_Tamanho_Resp\": len(resp_gen),\n",
        "\n",
        "        # Dados do Especialista\n",
        "        \"Especialista_Output\": resp_esp,\n",
        "        \"Especialista_Tempo_Sec\": round(tempo_esp, 2),\n",
        "        \"Especialista_Tamanho_Resp\": len(resp_esp)\n",
        "\n",
        "    })\n",
        "\n",
        "    # Pausa de 1 segundo para não estourar limite da OpenAI (Rate Limit)\n",
        "    time.sleep(1)\n",
        "\n",
        "# --- EXPORTAÇÃO FINAL ---\n",
        "df_resultados = pd.DataFrame(resultados_finais)\n",
        "\n",
        "# Salva em arquivos reais\n",
        "df_resultados.to_excel(\"Resultados_Completos_Alloy.xlsx\", index=False)\n",
        "df_resultados.to_csv(\"Resultados_Completos_Alloy.csv\", index=False)\n",
        "\n",
        "print(\"\\n Processamento Concluído!\")\n",
        "print(\"Os arquivos 'Resultados_Completos_Alloy' (.xlsx e .csv) foram salvos na aba de arquivos à esquerda.\")\n",
        "\n",
        "display(df_resultados.head())"
      ],
      "metadata": {
        "id": "7S06do0w8vif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "JARGOES_TECNICOS = [\n",
        "    # Palavras reservadas do Alloy (se aparecerem, é erro grave de tradução)\n",
        "    \"sig \", \"fact \", \"pred \", \"assert \", \"check \", \"run \", \"fun \",\n",
        "    \"abstract\", \"extends\", \"univ\", \"iden\", \"none\", \"this\",\n",
        "    \"one\", \"lone\", \"some\", \"all\", \" set \", \"disj\", \"let \", \"iff\",\n",
        "    \"util/\", \"ordering\",\n",
        "\n",
        "    # Traduções literais que soam matemáticas demais\n",
        "    \"assinatura\", \"predicado\", \"axioma\", \"multiplicidade\",\n",
        "    \"instância\", \"tupla\", \"produto cartesiano\", \"fecho transitivo\",\n",
        "    \"bijective\", \"injetora\", \"sobrejetora\", \"succ\", \"prevs\"\n",
        "]\n",
        "\n",
        "# Lista EXPANDIDA de Negócios (Recompensa)\n",
        "# Foca em verbos de ação, consequências e termos de domínio\n",
        "TERMOS_DE_NEGOCIO = [\n",
        "    # Substantivos de Abstração\n",
        "    \"entidade\", \"objeto\", \"elemento\", \"item\", \"indivíduo\", \"pessoa\",\n",
        "    \"usuário\", \"sistema\", \"modelo\", \"estrutura\", \"cenário\", \"requisito\",\n",
        "\n",
        "    # Verbos e Termos de Regra/Consequência (O mais importante!)\n",
        "    \"regra\", \"restrição\", \"garante\", \"assegura\", \"impede\", \"evita\",\n",
        "    \"permite\", \"proibido\", \"obrigatório\", \"necessário\", \"possível\",\n",
        "    \"válido\", \"inválido\", \"consistente\", \"inconsistente\",\n",
        "\n",
        "    # Relações Humanas (Para os datasets específicos como Grandpa/AddressBook)\n",
        "    \"pai\", \"mãe\", \"filho\", \"avô\", \"ancestral\", \"descendente\",\n",
        "    \"contém\", \"pertence\", \"conexão\", \"vínculo\",\n",
        "\n",
        "    # Domínio: Sistemas Distribuídos (RingElection)\n",
        "    \"processo\", \"nó\", \"servidor\", \"computador\",\n",
        "    \"mensagem\", \"comunicação\", \"envio\", \"troca\",\n",
        "    \"eleição\", \"líder\", \"coordenador\", \"escolhido\",\n",
        "    \"anel\", \"círculo\", \"vizinho\", \"sucessor\",\n",
        "    \"tempo\", \"passo\", \"estado\", \"evolução\"\n",
        "]\n",
        "\n",
        "def calcular_score_negocio(texto):\n",
        "    texto_lower = texto.lower()\n",
        "    score = 0\n",
        "    for termo in TERMOS_DE_NEGOCIO:\n",
        "        if termo in texto_lower:\n",
        "            score += 1\n",
        "    return score\n",
        "\n",
        "def calcular_penalidade_jargao(texto):\n",
        "    texto_lower = texto.lower()\n",
        "    score = 0\n",
        "    for termo in JARGOES_TECNICOS:\n",
        "        if termo in texto_lower:\n",
        "            score += 1\n",
        "    return score\n",
        "\n",
        "# Função Flesch adaptada para Português (Martins & Forghieri)\n",
        "def flesch_pt(texto):\n",
        "    try:\n",
        "        palavras = texto.split()\n",
        "        num_palavras = len(palavras)\n",
        "        if num_palavras == 0: return 0\n",
        "\n",
        "        # Estimativa simples de sílabas (contando vogais como aproximação rápida)\n",
        "        vogais = \"aeiouáéíóúàèìòùâêîôûãõ\"\n",
        "        num_silabas = sum(1 for char in texto.lower() if char in vogais)\n",
        "        num_frases = texto.count('.') + texto.count('!') + texto.count('?')\n",
        "        if num_frases == 0: num_frases = 1\n",
        "\n",
        "        # Fórmula Flesch Adaptada\n",
        "        score = 248.835 - (1.015 * (num_palavras / num_frases)) - (84.6 * (num_silabas / num_palavras))\n",
        "        return max(0, min(100, score)) # Trava entre 0 e 100\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "resultados = []\n",
        "\n",
        "print(f\"Iniciando Benchmark em {len(df_alloy)} arquivos...\")\n",
        "\n",
        "for index, row in tqdm(df_alloy.iterrows(), total=df_alloy.shape[0]):\n",
        "    arquivo = row['arquivo']\n",
        "    codigo = row['codigo_alloy']\n",
        "\n",
        "    # 1. GENERALISTA (GPT-4o Zero-Shot)\n",
        "    start = time.time()\n",
        "    resp_gen = generalist_model(codigo)\n",
        "    tempo_gen = time.time() - start\n",
        "\n",
        "    # 2. ESPECIALISTA (GPT-4o System Prompt)\n",
        "    start = time.time()\n",
        "    resp_esp = specialist_model(codigo)\n",
        "    tempo_esp = time.time() - start\n",
        "\n",
        "    # Coleta de Métricas\n",
        "    resultados.append({\n",
        "        \"Arquivo\": arquivo,\n",
        "\n",
        "        # Generalista\n",
        "        \"Gen_Score_Negocio\": calcular_score_negocio(resp_gen), # Quanto ele explicou o conceito?\n",
        "        \"Gen_Jargao\": calcular_penalidade_jargao(resp_gen),    # Quanto ele usou código no meio do texto?\n",
        "        \"Gen_Leiturabilidade_PT\": flesch_pt(resp_gen),         # Quão fácil é ler?\n",
        "\n",
        "        # Especialista\n",
        "        \"Esp_Score_Negocio\": calcular_score_negocio(resp_esp),\n",
        "        \"Esp_Jargao\": calcular_penalidade_jargao(resp_esp),\n",
        "        \"Esp_Leiturabilidade_PT\": flesch_pt(resp_esp),\n",
        "\n",
        "        # Textos\n",
        "        \"Gen_Texto\": resp_gen,\n",
        "        \"Esp_Texto\": resp_esp\n",
        "    })\n",
        "\n",
        "    time.sleep(1) # Pausa para não bloquear a API\n",
        "\n",
        "# Salvar e Mostrar\n",
        "df_final = pd.DataFrame(resultados)\n",
        "df_final.to_excel(\"Resultado_Final_Metricas.xlsx\", index=False)\n",
        "\n",
        "print(\"Análise concluída! Baixe o arquivo 'Resultado_Final_Metricas.xlsx'.\")\n",
        "display(df_final.head())"
      ],
      "metadata": {
        "id": "vlckP-PnFtai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q textstat\n",
        "\n",
        "import textstat\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Calculando Complexidade de Leitura (Flesch Score)...\")\n",
        "\n",
        "resultados_leitura = []\n",
        "\n",
        "\n",
        "for index, row in df_final.iterrows():\n",
        "    texto_gen = row['Gen_Texto']\n",
        "    texto_esp = row['Esp_Texto']\n",
        "\n",
        "    score_gen = flesch_pt(texto_gen)\n",
        "    score_esp = flesch_pt(texto_esp)\n",
        "\n",
        "    resultados_leitura.append({\n",
        "        \"Arquivo\": row['Arquivo'],\n",
        "        \"Gemini_Leiturabilidade\": score_gen,\n",
        "        \"Especialista_Leiturabilidade\": score_esp,\n",
        "        \"Vencedor_Clareza\": \"Generalista\" if score_gen > score_esp else \"Especialista\"\n",
        "    })\n",
        "\n",
        "df_leitura = pd.DataFrame(resultados_leitura)\n",
        "\n",
        "print(\"\\nCálculo de Leiturabilidade Concluído!\")\n",
        "display(df_leitura)"
      ],
      "metadata": {
        "id": "rtMYHNksFfud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}